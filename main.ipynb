{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.39.0 Augmentor editdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lra1wrMZtWV0",
        "outputId": "fc345efc-f034-4fdf-a46b-5acd895cea1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.39.0\n",
            "  Downloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Augmentor\n",
            "  Downloading Augmentor-0.2.12-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.0)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (4.66.5)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from Augmentor) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (2024.8.30)\n",
            "Downloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Augmentor-0.2.12-py2.py3-none-any.whl (38 kB)\n",
            "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Augmentor, tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed Augmentor-0.2.12 tokenizers-0.15.2 transformers-4.39.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==2.5 ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1BP_ngLtadE",
        "outputId": "22724212-ec46-4356-88e5-d5006fe9d982"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.5\n",
            "  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.23-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy==1.13.1 (from torch==2.5)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision>=0.9.0 (from ultralytics)\n",
            "  Downloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.23-py3-none-any.whl (877 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.6/877.6 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, ultralytics-thop, torchvision, ultralytics\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1+cu121\n",
            "    Uninstalling torch-2.4.1+cu121:\n",
            "      Successfully uninstalled torch-2.4.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.19.1+cu121\n",
            "    Uninstalling torchvision-0.19.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.19.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.17 requires torch<2.5,>=1.10, but you have torch 2.5.0 which is incompatible.\n",
            "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.0 torchvision-0.20.0 triton-3.1.0 ultralytics-8.3.23 ultralytics-thop-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wooZA-aPsQMD",
        "outputId": "1a630da1-c4d1-4227-cf1c-dd14054edaf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer layers: 2\n",
            "transformer heads: 4\n",
            "hidden dim: 512\n",
            "num classes: 92\n",
            "backbone: conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)\n",
            "dropout: 0.2\n",
            "19,838,174 trainable parameters\n",
            "Enter path\n",
            "1_pass_1.png\n",
            "1_pass_1.png\n",
            "\n",
            "image 1/1 /content/1_pass_1.png: 640x480 19 not_signs, 1 sign, 144.1ms\n",
            "Speed: 3.4ms preprocess, 144.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mdetected\u001b[0m\n",
            "[[[564.6967163085938, 1045.0654296875], [210.4940185546875, 64.945068359375]], [[742.272216796875, 187.37490844726562], [107.25115966796875, 43.45683288574219]], [[639.8463134765625, 1041.1761474609375], [375.00360107421875, 70.640380859375]], [[406.02032470703125, 999.91650390625], [115.267822265625, 48.26531982421875]], [[576.8048095703125, 765.8109130859375], [249.61666870117188, 66.5152587890625]], [[183.2305450439453, 446.08441162109375], [183.80892944335938, 113.84066772460938]], [[543.5894775390625, 882.1089477539062], [240.73626708984375, 55.653564453125]], [[267.8492736816406, 194.3089599609375], [215.689453125, 64.29965209960938]], [[518.5321655273438, 195.34548950195312], [238.0396728515625, 60.63056945800781]], [[309.5284118652344, 242.46510314941406], [314.5657958984375, 50.470611572265625]], [[665.92724609375, 242.24551391601562], [243.344482421875, 41.83235168457031]], [[700.2841186523438, 292.3075256347656], [206.8282470703125, 44.94488525390625]], [[394.9449462890625, 154.95631408691406], [242.5352783203125, 66.18209075927734]], [[589.929931640625, 933.2787475585938], [388.7536315917969, 52.8656005859375]], [[508.19024658203125, 1088.6424560546875], [321.0772705078125, 48.43603515625]], [[256.26385498046875, 299.1544189453125], [197.3598175048828, 40.612060546875]], [[716.9022216796875, 985.1146240234375], [269.57843017578125, 53.25262451171875]], [[686.8961181640625, 503.74176025390625], [157.06201171875, 59.2840576171875]], [[710.1586303710938, 141.34036254882812], [310.881103515625, 51.94984436035156]], [[703.5232543945312, 1044.32763671875], [245.160400390625, 64.4019775390625]]]\n",
            "459.69671630859375 669.6967163085938 1013.0654296875 1077.0654296875\n",
            "689.272216796875 795.272216796875 166.37490844726562 208.37490844726562\n",
            "452.8463134765625 826.8463134765625 1006.1761474609375 1076.1761474609375\n",
            "349.02032470703125 463.02032470703125 975.91650390625 1023.91650390625\n",
            "452.8048095703125 700.8048095703125 732.8109130859375 798.8109130859375\n",
            "92.23054504394531 274.2305450439453 390.08441162109375 502.08441162109375\n",
            "423.5894775390625 663.5894775390625 855.1089477539062 909.1089477539062\n",
            "160.84927368164062 374.8492736816406 162.3089599609375 226.3089599609375\n",
            "399.53216552734375 637.5321655273438 165.34548950195312 225.34548950195312\n",
            "152.52841186523438 466.5284118652344 217.46510314941406 267.46510314941406\n",
            "544.92724609375 786.92724609375 222.24551391601562 262.2455139160156\n",
            "597.2841186523438 803.2841186523438 270.3075256347656 314.3075256347656\n",
            "273.9449462890625 515.9449462890625 121.95631408691406 187.95631408691406\n",
            "395.929931640625 783.929931640625 907.2787475585938 959.2787475585938\n",
            "348.19024658203125 668.1902465820312 1064.6424560546875 1112.6424560546875\n",
            "158.26385498046875 354.26385498046875 279.1544189453125 319.1544189453125\n",
            "582.9022216796875 850.9022216796875 959.1146240234375 1011.1146240234375\n",
            "608.8961181640625 764.8961181640625 474.74176025390625 532.7417602539062\n",
            "555.1586303710938 865.1586303710938 116.34036254882812 166.34036254882812\n",
            "581.5232543945312 825.5232543945312 1012.32763671875 1076.32763671875\n",
            "Папка успешно создана.\n",
            "20 20 20\n",
            "Файл <PIL.Image.Image image mode=RGBA size=320x48 at 0x7FF8A3388040>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=310x50 at 0x7FF8A33896C0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=374x70 at 0x7FF8A3388250>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=314x50 at 0x7FF8AB2C7DC0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=210x64 at 0x7FF8A73AFD90>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=214x64 at 0x7FF8A3388760>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=196x40 at 0x7FF8A3389300>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=268x52 at 0x7FF8A33893C0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=238x60 at 0x7FF8A33881F0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=106x42 at 0x7FF8A33883A0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=156x58 at 0x7FF8A338B9A0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=244x64 at 0x7FF8A3388550>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=114x48 at 0x7FF8A33894B0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=182x112 at 0x7FF8A3388160>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=388x52 at 0x7FF8A3389390>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=242x66 at 0x7FF8A3388070>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=206x44 at 0x7FF8A33884F0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=242x40 at 0x7FF8A3389840>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=240x54 at 0x7FF8A338B8E0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=248x66 at 0x7FF8A3388310>.png удалён.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import pandas as pd\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import Augmentor\n",
        "from torch import nn\n",
        "from torch.nn import Conv2d, MaxPool2d, BatchNorm2d, LeakyReLU\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import editdistance\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import random\n",
        "import string\n",
        "import math\n",
        "from collections import Counter\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def detect(path,model):\n",
        "  res = det_model.predict(source=path, project='.',name='detected', exist_ok=True, save=True, show=False, show_labels=False, show_conf=False, conf=0.5)\n",
        "\n",
        "  image = Image.open(path)\n",
        "\n",
        "  array_of_boxes = []\n",
        "  for b in res[0].boxes.xywh:\n",
        "    b1 = b.tolist()\n",
        "    array_of_boxes.append([[b1[0],b1[1]],[b1[2],b1[3]]])\n",
        "  sign = res[0].boxes.cls.tolist()\n",
        "  return array_of_boxes,sign\n",
        "\n",
        "det_model = YOLO(\"best.pt\")\n",
        "#res = det_model.predict(source=\"1_pass_1.png\", project='.',name='detected', exist_ok=True, save=True, show=True, show_labels=True, show_conf=False, conf=0.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_rectangles(image, rectangles):\n",
        "    extracted_images = []\n",
        "\n",
        "    for  har in rectangles:\n",
        "        xc = har[0][0]\n",
        "        yc = har[0][1]\n",
        "        w = har[1][0]\n",
        "        h = har[1][1]\n",
        "        x1 = xc - w// 2\n",
        "        y1 = yc - h// 2\n",
        "        x2 = xc + w // 2\n",
        "        y2 = yc + h // 2\n",
        "        print(x1,x2,y1,y2)\n",
        "        cropped_img = image.crop((x1, y1, x2, y2))\n",
        "        cropped_img.show()\n",
        "        extracted_images.append(cropped_img)\n",
        "\n",
        "    return extracted_images\n",
        "\n",
        "\n",
        "def clear(directory):\n",
        "  for filename in os.listdir(directory):\n",
        "    file_path = os.path.join(directory, filename)\n",
        "    try:\n",
        "        # Если это файл, то удалить его\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "            print(f\"Файл {filename} удалён.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Не удалось удалить {filename}: {e}\")\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.scale = torch.nn.Parameter(torch.ones(1))\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(\n",
        "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.scale * self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "# convert images and labels into defined data structures\n",
        "def process_data(image_dir, labels_dir, ignore=[]):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    image_dir : str\n",
        "      path to directory with images\n",
        "    labels_dir : str\n",
        "      path to tsv file with labels\n",
        "    returns\n",
        "    ---\n",
        "    img2label : dict\n",
        "      keys are names of images and values are correspondent labels\n",
        "    chars : list\n",
        "      all unique chars used in data\n",
        "    all_labels : list\n",
        "    \"\"\"\n",
        "\n",
        "    chars = []\n",
        "    img2label = dict()\n",
        "\n",
        "    raw = open(labels_dir, 'r', encoding='utf-8').read()\n",
        "    temp = raw.split('\\n')\n",
        "    for t in temp:\n",
        "        try:\n",
        "            x = t.split('\\t')\n",
        "            flag = False\n",
        "            for item in ignore:\n",
        "                if item in x[1]:\n",
        "                    flag = True\n",
        "            if flag == False:\n",
        "                img2label[image_dir + x[0]] = x[1]\n",
        "                for char in x[1]:\n",
        "                    if char not in chars:\n",
        "                        chars.append(char)\n",
        "        except:\n",
        "            print('ValueError:', x)\n",
        "            pass\n",
        "\n",
        "    all_labels = sorted(list(set(list(img2label.values()))))\n",
        "    chars.sort()\n",
        "    chars = ['PAD', 'SOS'] + chars + ['EOS']\n",
        "\n",
        "    return img2label, chars, all_labels\n",
        "\n",
        "\n",
        "# TRANSLATE INDICIES TO TEXT\n",
        "def indicies_to_text(indexes, idx2char):\n",
        "    text = \"\".join([idx2char[i] for i in indexes])\n",
        "    text = text.replace('EOS', '').replace('PAD', '').replace('SOS', '')\n",
        "    return text\n",
        "\n",
        "\n",
        "# COMPUTE CHARACTER ERROR RATE\n",
        "def char_error_rate(p_seq1, p_seq2):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    p_seq1 : str\n",
        "    p_seq2 : str\n",
        "    returns\n",
        "    ---\n",
        "    cer : float\n",
        "    \"\"\"\n",
        "    p_vocab = set(p_seq1 + p_seq2)\n",
        "    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n",
        "    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n",
        "    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n",
        "    return editdistance.eval(''.join(c_seq1),\n",
        "                             ''.join(c_seq2)) / max(len(c_seq1), len(c_seq2))\n",
        "\n",
        "\n",
        "# RESIZE AND NORMALIZE IMAGE\n",
        "def process_image(img):\n",
        "    \"\"\"\n",
        "    params:\n",
        "    ---\n",
        "    img : np.array\n",
        "    returns\n",
        "    ---\n",
        "    img : np.array\n",
        "    \"\"\"\n",
        "    w, h, _ = img.shape\n",
        "    new_w = HEIGHT\n",
        "    new_h = int(h * (new_w / w))\n",
        "    img = cv2.resize(img, (new_h, new_w))\n",
        "    w, h, _ = img.shape\n",
        "\n",
        "    img = img.astype('float32')\n",
        "\n",
        "    new_h = WIDTH\n",
        "    if h < new_h:\n",
        "        add_zeros = np.full((w, new_h - h, 3), 255)\n",
        "        img = np.concatenate((img, add_zeros), axis=1)\n",
        "\n",
        "    if h > new_h:\n",
        "        img = cv2.resize(img, (new_h, new_w))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# GENERATE IMAGES FROM FOLDER\n",
        "def generate_data(img_paths):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    names : list of str\n",
        "        paths to images\n",
        "    returns\n",
        "    ---\n",
        "    data_images : list of np.array\n",
        "        images in np.array format\n",
        "    \"\"\"\n",
        "    data_images = []\n",
        "    for path in tqdm(img_paths):\n",
        "        img = np.asarray(Image.open(path).convert('RGB'))\n",
        "        try:\n",
        "            img = process_image(img)\n",
        "            data_images.append(img.astype('uint8'))\n",
        "        except:\n",
        "            print(path)\n",
        "            img = process_image(img)\n",
        "    return data_images\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, loader, case=True, punct=True):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    model : nn.Module\n",
        "    criterion : nn.Object\n",
        "    loader : torch.utils.data.DataLoader\n",
        "\n",
        "    returns\n",
        "    ---\n",
        "    epoch_loss / len(loader) : float\n",
        "        overall loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    metrics = {'loss': 0, 'wer': 0, 'cer': 0}\n",
        "    result = {'true': [], 'predicted': [], 'wer': []}\n",
        "    with torch.no_grad():\n",
        "        for (src, trg) in loader:\n",
        "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "            logits = model(src, trg[:-1, :])\n",
        "            loss = criterion(logits.view(-1, logits.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n",
        "            out_indexes = model.predict(src)\n",
        "\n",
        "            true_phrases = [indicies_to_text(trg.T[i][1:], ALPHABET) for i in range(BATCH_SIZE)]\n",
        "            pred_phrases = [indicies_to_text(out_indexes[i], ALPHABET) for i in range(BATCH_SIZE)]\n",
        "\n",
        "            if not case:\n",
        "                true_phrases = [phrase.lower() for phrase in true_phrases]\n",
        "                pred_phrases = [phrase.lower() for phrase in pred_phrases]\n",
        "            if not punct:\n",
        "                true_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n",
        "                                for phrase in true_phrases]\n",
        "                pred_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n",
        "                                for phrase in pred_phrases]\n",
        "\n",
        "            metrics['loss'] += loss.item()\n",
        "            metrics['cer'] += sum([char_error_rate(true_phrases[i], pred_phrases[i]) \\\n",
        "                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n",
        "            metrics['wer'] += sum([int(true_phrases[i] != pred_phrases[i]) \\\n",
        "                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n",
        "\n",
        "            for i in range(len(true_phrases)):\n",
        "              result['true'].append(true_phrases[i])\n",
        "              result['predicted'].append(pred_phrases[i])\n",
        "              result['wer'].append(char_error_rate(true_phrases[i], pred_phrases[i]))\n",
        "\n",
        "    for key in metrics.keys():\n",
        "      metrics[key] /= len(loader)\n",
        "\n",
        "    return metrics, result\n",
        "\n",
        "\n",
        "# MAKE PREDICTION\n",
        "def prediction(model, test_dir, char2idx, idx2char):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    model : nn.Module\n",
        "    test_dir : str\n",
        "        path to directory with images\n",
        "    char2idx : dict\n",
        "        map from chars to indicies\n",
        "    id2char : dict\n",
        "        map from indicies to chars\n",
        "\n",
        "    returns\n",
        "    ---\n",
        "    preds : dict\n",
        "        key : name of image in directory\n",
        "        value : dict with keys ['p_value', 'predicted_label']\n",
        "    \"\"\"\n",
        "    preds = {}\n",
        "    os.makedirs('/output', exist_ok=True)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for filename in os.listdir(test_dir):\n",
        "            file_path = os.path.join(test_dir, filename)\n",
        "            if (os.path.isfile(file_path)):\n",
        "              img = Image.open(test_dir + filename).convert('RGB')\n",
        "\n",
        "              img = process_image(np.asarray(img)).astype('uint8')\n",
        "              img = img / img.max()\n",
        "              img = np.transpose(img, (2, 0, 1))\n",
        "\n",
        "              src = torch.FloatTensor(img).unsqueeze(0).to(DEVICE)\n",
        "              if CHANNELS == 1:\n",
        "                src = transforms.Grayscale(CHANNELS)(src)\n",
        "              out_indexes = model.predict(src)\n",
        "              pred = indicies_to_text(out_indexes[0], idx2char)\n",
        "              preds[filename] = pred\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __init__(self, X_type=None, Y_type=None):\n",
        "        self.X_type = X_type\n",
        "\n",
        "    def __call__(self, X):\n",
        "        X = X.transpose((2, 0, 1))\n",
        "        X = torch.from_numpy(X)\n",
        "        if self.X_type is not None:\n",
        "            X = X.type(self.X_type)\n",
        "        return X\n",
        "\n",
        "\n",
        "def log_config(model):\n",
        "    print('transformer layers: {}'.format(model.enc_layers))\n",
        "    print('transformer heads: {}'.format(model.transformer.nhead))\n",
        "    print('hidden dim: {}'.format(model.decoder.embedding_dim))\n",
        "    print('num classes: {}'.format(model.decoder.num_embeddings))\n",
        "    print('backbone: {}'.format(model.backbone_name))\n",
        "    print('dropout: {}'.format(model.pos_encoder.dropout.p))\n",
        "    print(f'{count_parameters(model):,} trainable parameters')\n",
        "\n",
        "\n",
        "def log_metrics(metrics, path_to_logs=None):\n",
        "    if path_to_logs != None:\n",
        "      f = open(path_to_logs, 'a')\n",
        "    if metrics['epoch'] == 1:\n",
        "      if path_to_logs != None:\n",
        "        f.write('Epoch\\tTrain_loss\\tValid_loss\\tCER\\tWER\\tTime\\n')\n",
        "      print('Epoch   Train_loss   Valid_loss   CER   WER    Time    LR')\n",
        "      print('-----   -----------  ----------   ---   ---    ----    ---')\n",
        "    print('{:02d}       {:.2f}         {:.2f}       {:.2f}   {:.2f}   {:.2f}   {:.7f}'.format(\\\n",
        "        metrics['epoch'], metrics['train_loss'], metrics['loss'], metrics['cer'], \\\n",
        "        metrics['wer'], metrics['time'], metrics['lr']))\n",
        "    if path_to_logs != None:\n",
        "      f.write(str(metrics['epoch'])+'\\t'+str(metrics['train_loss'])+'\\t'+str(metrics['loss'])+'\\t'+str(metrics['cer'])+'\\t'+str(metrics['wer'])+'\\t'+str(metrics['time'])+'\\n')\n",
        "      f.close()\n",
        "\n",
        "def main(path,det_model,rec_model):\n",
        "  print(path)\n",
        "  array_of_boxes,sign = detect(path,det_model)\n",
        "  print(array_of_boxes)\n",
        "  image = Image.open(path)\n",
        "  ans = extract_rectangles(image, array_of_boxes)\n",
        "  name_image = path[path.rfind('/')+1:]\n",
        "  directory = './parts of ' + name_image + '/'\n",
        "\n",
        "  try:\n",
        "      os.mkdir(directory)\n",
        "      print(\"Папка успешно создана.\")\n",
        "  except FileExistsError:\n",
        "      print(\"Папка уже существует.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Произошла ошибка: {e}\")\n",
        "\n",
        "  for i in ans:\n",
        "    i.save(directory+ str(i) + \".png\")\n",
        "\n",
        "  preds = prediction(rec_model, directory, char2idx, idx2char)\n",
        "\n",
        "  labels = []\n",
        "\n",
        "  for i in preds.values():\n",
        "    labels.append(i)\n",
        "\n",
        "  print(len(sign), len(labels),len(array_of_boxes))\n",
        "\n",
        "  df = pd.DataFrame({\"coordinates\":array_of_boxes,\"content\":labels,\"signature\":sign})\n",
        "  df[\"signature\"] = df[\"signature\"].astype(bool)\n",
        "  df.to_json(\"./temp/\"+name_image[:name_image.find('.')]+ \".json\")\n",
        "\n",
        "  clear(directory)\n",
        "  os.rmdir(directory)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.enc_layers = enc_layers\n",
        "        self.dec_layers = dec_layers\n",
        "        self.backbone_name = 'conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)'\n",
        "\n",
        "        self.conv0 = Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv1 = Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv2 = Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n",
        "        self.conv3 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv4 = Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n",
        "        self.conv5 = Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv6 = Conv2d(512, 512, kernel_size=(2, 1), stride=(1, 1))\n",
        "\n",
        "        self.pool1 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        self.pool3 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        self.pool5 = MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
        "\n",
        "        self.bn0 = BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn1 = BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn2 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn3 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn4 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn5 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn6 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "        self.activ = LeakyReLU()\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(hidden, dropout)\n",
        "        self.decoder = nn.Embedding(outtoken, hidden)\n",
        "        self.pos_decoder = PositionalEncoding(hidden, dropout)\n",
        "        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n",
        "                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden, outtoken)\n",
        "        self.src_mask = None\n",
        "        self.trg_mask = None\n",
        "        self.memory_mask = None\n",
        "\n",
        "        log_config(self)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = torch.triu(torch.ones(sz, sz, device=DEVICE), 1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        return mask\n",
        "\n",
        "    def make_len_mask(self, inp):\n",
        "        return (inp == 0).transpose(0, 1)\n",
        "\n",
        "    def _get_features(self, src):\n",
        "        '''\n",
        "        params\n",
        "        ---\n",
        "        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
        "            B - batch, C - channel, H - height, W - width\n",
        "        returns\n",
        "        ---\n",
        "        x : Tensor : [W,B,CH]\n",
        "        '''\n",
        "        x = self.activ(self.bn0(self.conv0(src)))\n",
        "        x = self.pool1(self.activ(self.bn1(self.conv1(x))))\n",
        "        x = self.activ(self.bn2(self.conv2(x)))\n",
        "        x = self.pool3(self.activ(self.bn3(self.conv3(x))))\n",
        "        x = self.activ(self.bn4(self.conv4(x)))\n",
        "        x = self.pool5(self.activ(self.bn5(self.conv5(x))))\n",
        "        x = self.activ(self.bn6(self.conv6(x)))\n",
        "        x = x.permute(0, 3, 1, 2).flatten(2).permute(1, 0, 2)\n",
        "        return x\n",
        "\n",
        "    def predict(self, batch):\n",
        "        '''\n",
        "        params\n",
        "        ---\n",
        "        batch : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
        "            B - batch, C - channel, H - height, W - width\n",
        "\n",
        "        returns\n",
        "        ---\n",
        "        result : List [64, -1] : [B, -1]\n",
        "            preticted sequences of tokens' indexes\n",
        "        '''\n",
        "        result = []\n",
        "        for item in batch:\n",
        "          x = self._get_features(item.unsqueeze(0))\n",
        "          memory = self.transformer.encoder(self.pos_encoder(x))\n",
        "          out_indexes = [ALPHABET.index('SOS'), ]\n",
        "          for i in range(100):\n",
        "              trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(DEVICE)\n",
        "              output = self.fc_out(self.transformer.decoder(self.pos_decoder(self.decoder(trg_tensor)), memory))\n",
        "\n",
        "              out_token = output.argmax(2)[-1].item()\n",
        "              out_indexes.append(out_token)\n",
        "              if out_token == ALPHABET.index('EOS'):\n",
        "                  break\n",
        "          result.append(out_indexes)\n",
        "        return result\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        '''\n",
        "        params\n",
        "        ---\n",
        "        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
        "            B - batch, C - channel, H - height, W - width\n",
        "        trg : Tensor [13, 64] : [L,B]\n",
        "            L - max length of label\n",
        "        '''\n",
        "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
        "            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n",
        "\n",
        "        x = self._get_features(src)\n",
        "        src_pad_mask = self.make_len_mask(x[:, :, 0])\n",
        "        src = self.pos_encoder(x)\n",
        "        trg_pad_mask = self.make_len_mask(trg)\n",
        "        trg = self.decoder(trg)\n",
        "        trg = self.pos_decoder(trg)\n",
        "\n",
        "        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n",
        "                                  memory_mask=self.memory_mask,\n",
        "                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n",
        "                                  memory_key_padding_mask=src_pad_mask)\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "MODEL = 'model2'\n",
        "HIDDEN = 512\n",
        "ENC_LAYERS = 2\n",
        "DEC_LAYERS = 2\n",
        "N_HEADS = 4\n",
        "LENGTH = 42\n",
        "ALPHABET = ['PAD', 'SOS', ' ', '!', '\"', '%', '(', ')', ',', '-', '.', '/',\n",
        "            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?',\n",
        "            '[', ']', '«', '»', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И',\n",
        "            'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х',\n",
        "            'Ц', 'Ч', 'Ш', 'Щ', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е',\n",
        "            'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т',\n",
        "            'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я',\n",
        "            'ё', 'EOS']\n",
        "\n",
        "### TRAINING ###\n",
        "BATCH_SIZE = 16\n",
        "DROPOUT = 0.2\n",
        "N_EPOCHS = 10\n",
        "CHECKPOINT_FREQ = 10 # save checkpoint every 10 epochs\n",
        "DEVICE = 'cpu' # or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "RANDOM_SEED = 42\n",
        "SCHUDULER_ON = True # \"ReduceLROnPlateau\"\n",
        "PATIENCE = 5 # for ReduceLROnPlateau\n",
        "OPTIMIZER_NAME = 'Adam' # or \"SGD\"\n",
        "LR = 2e-6\n",
        "\n",
        "### TESTING ###\n",
        "CASE = False # is case taken into account or not while evaluating\n",
        "PUNCT = False # are punctuation marks taken into account\n",
        "\n",
        "### INPUT IMAGE PARAMETERS ###\n",
        "WIDTH = 256\n",
        "HEIGHT = 64\n",
        "CHANNELS = 1 # 3\n",
        "\n",
        "\n",
        "char2idx = {char: idx for idx, char in enumerate(ALPHABET)}\n",
        "idx2char = {idx: char for idx, char in enumerate(ALPHABET)}\n",
        "\n",
        "rec_model = TransformerModel(len(ALPHABET), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,\n",
        "                          nhead=N_HEADS, dropout=DROPOUT).to(DEVICE)\n",
        "rec_model.load_state_dict(torch.load('./model.pt',map_location = torch.device('cpu')))\n",
        "\n",
        "\n",
        "#f = open(\"./temp/image_name.txt\",'r')\n",
        "print(\"Enter path\")\n",
        "path = input()\n",
        "main(path,det_model,rec_model)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clear(\"parts of 1_pass_1.png/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ToPB_bT0u4D",
        "outputId": "819b1ac3-77ac-4668-9a10-067167928373"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл <PIL.Image.Image image mode=RGBA size=114x48 at 0x7FF8AB2933A0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=244x64 at 0x7FF8A3EE2500>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=214x64 at 0x7FF8A3EE2D10>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=206x44 at 0x7FF8AB290B20>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=114x48 at 0x7FF8A3EE22C0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=314x50 at 0x7FF8A3EE0760>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=106x42 at 0x7FF8A716D180>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=320x48 at 0x7FF8A3EE2800>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=156x58 at 0x7FF8A716E440>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=242x40 at 0x7FF8A3EE16F0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=310x50 at 0x7FF8A716EB90>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=310x50 at 0x7FF8A3EE3790>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=242x66 at 0x7FF8A716E650>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=244x64 at 0x7FF8A716D0F0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=320x48 at 0x7FF8A3EE0A00>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=374x70 at 0x7FF8A3EE2890>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=156x58 at 0x7FF8A3EE3FD0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=196x40 at 0x7FF8A716C5E0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=268x52 at 0x7FF8A3EE0280>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=242x40 at 0x7FF8A716CD00>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=242x66 at 0x7FF8AB292650>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=240x54 at 0x7FF8A3EE3040>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=106x42 at 0x7FF8A3EE3A30>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=210x64 at 0x7FF8AB290490>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=206x44 at 0x7FF8A3EE2C80>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=182x112 at 0x7FF8A716F8E0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=388x52 at 0x7FF8A3EE0340>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=240x54 at 0x7FF8A3EE0F40>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=210x64 at 0x7FF8A3EE14E0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=374x70 at 0x7FF8A716EDA0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=196x40 at 0x7FF8A3EE1F60>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=314x50 at 0x7FF8A716D660>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=214x64 at 0x7FF8A716E200>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=238x60 at 0x7FF8A716F310>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=248x66 at 0x7FF8A3EE3220>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=248x66 at 0x7FF8A716D330>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=388x52 at 0x7FF8A3EE3550>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=268x52 at 0x7FF8A716E4A0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=238x60 at 0x7FF8AB293580>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=182x112 at 0x7FF8A3EE0B50>.png удалён.\n"
          ]
        }
      ]
    }
  ]
}