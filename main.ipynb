{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.39.0 Augmentor editdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lra1wrMZtWV0",
        "outputId": "3046a96c-5f29-484b-de61-526f9c149ac6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.39.0\n",
            "  Downloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m133.1/134.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Augmentor\n",
            "  Downloading Augmentor-0.2.12-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.0)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (4.66.5)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from Augmentor) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (2024.8.30)\n",
            "Downloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Augmentor-0.2.12-py2.py3-none-any.whl (38 kB)\n",
            "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Augmentor, tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed Augmentor-0.2.12 tokenizers-0.15.2 transformers-4.39.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==2.5 ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1BP_ngLtadE",
        "outputId": "e5b9022c-dc7a-418e-c816-4d409627f3d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.5 in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.23-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Downloading ultralytics-8.3.23-py3-none-any.whl (877 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.6/877.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.23 ultralytics-thop-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wooZA-aPsQMD",
        "outputId": "a1d73046-8a00-4bed-fc66-e842966d596b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer layers: 2\n",
            "transformer heads: 4\n",
            "hidden dim: 512\n",
            "num classes: 92\n",
            "backbone: conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)\n",
            "dropout: 0.2\n",
            "19,838,174 trainable parameters\n",
            "Enter path\n",
            "1_pass_1.png\n",
            "1_pass_1.png\n",
            "\n",
            "image 1/1 /content/1_pass_1.png: 640x480 19 not_signs, 1 sign, 150.4ms\n",
            "Speed: 3.2ms preprocess, 150.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mdetected\u001b[0m\n",
            "[[[564.6967163085938, 1045.0654296875], [210.4940185546875, 64.945068359375]], [[742.272216796875, 187.37490844726562], [107.25115966796875, 43.45683288574219]], [[639.8463134765625, 1041.1761474609375], [375.00360107421875, 70.640380859375]], [[406.02032470703125, 999.91650390625], [115.267822265625, 48.26531982421875]], [[576.8048095703125, 765.8109130859375], [249.61666870117188, 66.5152587890625]], [[183.2305450439453, 446.08441162109375], [183.80892944335938, 113.84066772460938]], [[543.5894775390625, 882.1089477539062], [240.73626708984375, 55.653564453125]], [[267.8492736816406, 194.3089599609375], [215.689453125, 64.29965209960938]], [[518.5321655273438, 195.34548950195312], [238.0396728515625, 60.63056945800781]], [[309.5284118652344, 242.46510314941406], [314.5657958984375, 50.470611572265625]], [[665.92724609375, 242.24551391601562], [243.344482421875, 41.83235168457031]], [[700.2841186523438, 292.3075256347656], [206.8282470703125, 44.94488525390625]], [[394.9449462890625, 154.95631408691406], [242.5352783203125, 66.18209075927734]], [[589.929931640625, 933.2787475585938], [388.7536315917969, 52.8656005859375]], [[508.19024658203125, 1088.6424560546875], [321.0772705078125, 48.43603515625]], [[256.26385498046875, 299.1544189453125], [197.3598175048828, 40.612060546875]], [[716.9022216796875, 985.1146240234375], [269.57843017578125, 53.25262451171875]], [[686.8961181640625, 503.74176025390625], [157.06201171875, 59.2840576171875]], [[710.1586303710938, 141.34036254882812], [310.881103515625, 51.94984436035156]], [[703.5232543945312, 1044.32763671875], [245.160400390625, 64.4019775390625]]]\n",
            "459.69671630859375 669.6967163085938 1013.0654296875 1077.0654296875\n",
            "689.272216796875 795.272216796875 166.37490844726562 208.37490844726562\n",
            "452.8463134765625 826.8463134765625 1006.1761474609375 1076.1761474609375\n",
            "349.02032470703125 463.02032470703125 975.91650390625 1023.91650390625\n",
            "452.8048095703125 700.8048095703125 732.8109130859375 798.8109130859375\n",
            "92.23054504394531 274.2305450439453 390.08441162109375 502.08441162109375\n",
            "423.5894775390625 663.5894775390625 855.1089477539062 909.1089477539062\n",
            "160.84927368164062 374.8492736816406 162.3089599609375 226.3089599609375\n",
            "399.53216552734375 637.5321655273438 165.34548950195312 225.34548950195312\n",
            "152.52841186523438 466.5284118652344 217.46510314941406 267.46510314941406\n",
            "544.92724609375 786.92724609375 222.24551391601562 262.2455139160156\n",
            "597.2841186523438 803.2841186523438 270.3075256347656 314.3075256347656\n",
            "273.9449462890625 515.9449462890625 121.95631408691406 187.95631408691406\n",
            "395.929931640625 783.929931640625 907.2787475585938 959.2787475585938\n",
            "348.19024658203125 668.1902465820312 1064.6424560546875 1112.6424560546875\n",
            "158.26385498046875 354.26385498046875 279.1544189453125 319.1544189453125\n",
            "582.9022216796875 850.9022216796875 959.1146240234375 1011.1146240234375\n",
            "608.8961181640625 764.8961181640625 474.74176025390625 532.7417602539062\n",
            "555.1586303710938 865.1586303710938 116.34036254882812 166.34036254882812\n",
            "581.5232543945312 825.5232543945312 1012.32763671875 1076.32763671875\n",
            "Папка уже существует.\n",
            "20 20 20\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import pandas as pd\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import Augmentor\n",
        "from torch import nn\n",
        "from torch.nn import Conv2d, MaxPool2d, BatchNorm2d, LeakyReLU\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import editdistance\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import random\n",
        "import string\n",
        "import math\n",
        "from collections import Counter\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def detect(path,model):\n",
        "  res = det_model.predict(source=path, project='.',name='detected', exist_ok=True, save=True, show=False, show_labels=False, show_conf=False, conf=0.5)\n",
        "\n",
        "  image = Image.open(path)\n",
        "\n",
        "  array_of_boxes = []\n",
        "  for b in res[0].boxes.xywh:\n",
        "    b1 = b.tolist()\n",
        "    array_of_boxes.append([[b1[0],b1[1]],[b1[2],b1[3]]])\n",
        "  sign = res[0].boxes.cls.tolist()\n",
        "  return array_of_boxes,sign\n",
        "\n",
        "det_model = YOLO(\"best.pt\")\n",
        "#res = det_model.predict(source=\"1_pass_1.png\", project='.',name='detected', exist_ok=True, save=True, show=True, show_labels=True, show_conf=False, conf=0.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_rectangles(image, rectangles):\n",
        "    extracted_images = []\n",
        "\n",
        "    for  har in rectangles:\n",
        "        xc = har[0][0]\n",
        "        yc = har[0][1]\n",
        "        w = har[1][0]\n",
        "        h = har[1][1]\n",
        "        x1 = xc - w// 2\n",
        "        y1 = yc - h// 2\n",
        "        x2 = xc + w // 2\n",
        "        y2 = yc + h // 2\n",
        "        print(x1,x2,y1,y2)\n",
        "        cropped_img = image.crop((x1, y1, x2, y2))\n",
        "        cropped_img.show()\n",
        "        extracted_images.append(cropped_img)\n",
        "\n",
        "    return extracted_images\n",
        "\n",
        "\n",
        "def clear(directory):\n",
        "  for filename in os.listdir(directory):\n",
        "    file_path = os.path.join(directory, filename)\n",
        "    try:\n",
        "        # Если это файл, то удалить его\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "            print(f\"Файл {filename} удалён.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Не удалось удалить {filename}: {e}\")\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.scale = torch.nn.Parameter(torch.ones(1))\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(\n",
        "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.scale * self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "# convert images and labels into defined data structures\n",
        "def process_data(image_dir, labels_dir, ignore=[]):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    image_dir : str\n",
        "      path to directory with images\n",
        "    labels_dir : str\n",
        "      path to tsv file with labels\n",
        "    returns\n",
        "    ---\n",
        "    img2label : dict\n",
        "      keys are names of images and values are correspondent labels\n",
        "    chars : list\n",
        "      all unique chars used in data\n",
        "    all_labels : list\n",
        "    \"\"\"\n",
        "\n",
        "    chars = []\n",
        "    img2label = dict()\n",
        "\n",
        "    raw = open(labels_dir, 'r', encoding='utf-8').read()\n",
        "    temp = raw.split('\\n')\n",
        "    for t in temp:\n",
        "        try:\n",
        "            x = t.split('\\t')\n",
        "            flag = False\n",
        "            for item in ignore:\n",
        "                if item in x[1]:\n",
        "                    flag = True\n",
        "            if flag == False:\n",
        "                img2label[image_dir + x[0]] = x[1]\n",
        "                for char in x[1]:\n",
        "                    if char not in chars:\n",
        "                        chars.append(char)\n",
        "        except:\n",
        "            print('ValueError:', x)\n",
        "            pass\n",
        "\n",
        "    all_labels = sorted(list(set(list(img2label.values()))))\n",
        "    chars.sort()\n",
        "    chars = ['PAD', 'SOS'] + chars + ['EOS']\n",
        "\n",
        "    return img2label, chars, all_labels\n",
        "\n",
        "\n",
        "# TRANSLATE INDICIES TO TEXT\n",
        "def indicies_to_text(indexes, idx2char):\n",
        "    text = \"\".join([idx2char[i] for i in indexes])\n",
        "    text = text.replace('EOS', '').replace('PAD', '').replace('SOS', '')\n",
        "    return text\n",
        "\n",
        "\n",
        "# COMPUTE CHARACTER ERROR RATE\n",
        "def char_error_rate(p_seq1, p_seq2):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    p_seq1 : str\n",
        "    p_seq2 : str\n",
        "    returns\n",
        "    ---\n",
        "    cer : float\n",
        "    \"\"\"\n",
        "    p_vocab = set(p_seq1 + p_seq2)\n",
        "    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n",
        "    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n",
        "    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n",
        "    return editdistance.eval(''.join(c_seq1),\n",
        "                             ''.join(c_seq2)) / max(len(c_seq1), len(c_seq2))\n",
        "\n",
        "\n",
        "# RESIZE AND NORMALIZE IMAGE\n",
        "def process_image(img):\n",
        "    \"\"\"\n",
        "    params:\n",
        "    ---\n",
        "    img : np.array\n",
        "    returns\n",
        "    ---\n",
        "    img : np.array\n",
        "    \"\"\"\n",
        "    w, h, _ = img.shape\n",
        "    new_w = HEIGHT\n",
        "    new_h = int(h * (new_w / w))\n",
        "    img = cv2.resize(img, (new_h, new_w))\n",
        "    w, h, _ = img.shape\n",
        "\n",
        "    img = img.astype('float32')\n",
        "\n",
        "    new_h = WIDTH\n",
        "    if h < new_h:\n",
        "        add_zeros = np.full((w, new_h - h, 3), 255)\n",
        "        img = np.concatenate((img, add_zeros), axis=1)\n",
        "\n",
        "    if h > new_h:\n",
        "        img = cv2.resize(img, (new_h, new_w))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# GENERATE IMAGES FROM FOLDER\n",
        "def generate_data(img_paths):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    names : list of str\n",
        "        paths to images\n",
        "    returns\n",
        "    ---\n",
        "    data_images : list of np.array\n",
        "        images in np.array format\n",
        "    \"\"\"\n",
        "    data_images = []\n",
        "    for path in tqdm(img_paths):\n",
        "        img = np.asarray(Image.open(path).convert('RGB'))\n",
        "        try:\n",
        "            img = process_image(img)\n",
        "            data_images.append(img.astype('uint8'))\n",
        "        except:\n",
        "            print(path)\n",
        "            img = process_image(img)\n",
        "    return data_images\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, loader, case=True, punct=True):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    model : nn.Module\n",
        "    criterion : nn.Object\n",
        "    loader : torch.utils.data.DataLoader\n",
        "\n",
        "    returns\n",
        "    ---\n",
        "    epoch_loss / len(loader) : float\n",
        "        overall loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    metrics = {'loss': 0, 'wer': 0, 'cer': 0}\n",
        "    result = {'true': [], 'predicted': [], 'wer': []}\n",
        "    with torch.no_grad():\n",
        "        for (src, trg) in loader:\n",
        "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "            logits = model(src, trg[:-1, :])\n",
        "            loss = criterion(logits.view(-1, logits.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n",
        "            out_indexes = model.predict(src)\n",
        "\n",
        "            true_phrases = [indicies_to_text(trg.T[i][1:], ALPHABET) for i in range(BATCH_SIZE)]\n",
        "            pred_phrases = [indicies_to_text(out_indexes[i], ALPHABET) for i in range(BATCH_SIZE)]\n",
        "\n",
        "            if not case:\n",
        "                true_phrases = [phrase.lower() for phrase in true_phrases]\n",
        "                pred_phrases = [phrase.lower() for phrase in pred_phrases]\n",
        "            if not punct:\n",
        "                true_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n",
        "                                for phrase in true_phrases]\n",
        "                pred_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n",
        "                                for phrase in pred_phrases]\n",
        "\n",
        "            metrics['loss'] += loss.item()\n",
        "            metrics['cer'] += sum([char_error_rate(true_phrases[i], pred_phrases[i]) \\\n",
        "                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n",
        "            metrics['wer'] += sum([int(true_phrases[i] != pred_phrases[i]) \\\n",
        "                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n",
        "\n",
        "            for i in range(len(true_phrases)):\n",
        "              result['true'].append(true_phrases[i])\n",
        "              result['predicted'].append(pred_phrases[i])\n",
        "              result['wer'].append(char_error_rate(true_phrases[i], pred_phrases[i]))\n",
        "\n",
        "    for key in metrics.keys():\n",
        "      metrics[key] /= len(loader)\n",
        "\n",
        "    return metrics, result\n",
        "\n",
        "\n",
        "# MAKE PREDICTION\n",
        "def prediction(model, test_dir, char2idx, idx2char):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    model : nn.Module\n",
        "    test_dir : str\n",
        "        path to directory with images\n",
        "    char2idx : dict\n",
        "        map from chars to indicies\n",
        "    id2char : dict\n",
        "        map from indicies to chars\n",
        "\n",
        "    returns\n",
        "    ---\n",
        "    preds : dict\n",
        "        key : name of image in directory\n",
        "        value : dict with keys ['p_value', 'predicted_label']\n",
        "    \"\"\"\n",
        "    preds = {}\n",
        "    os.makedirs('/output', exist_ok=True)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for filename in os.listdir(test_dir):\n",
        "            file_path = os.path.join(test_dir, filename)\n",
        "            if (os.path.isfile(file_path)):\n",
        "              img = Image.open(test_dir + filename).convert('RGB')\n",
        "\n",
        "              img = process_image(np.asarray(img)).astype('uint8')\n",
        "              img = img / img.max()\n",
        "              img = np.transpose(img, (2, 0, 1))\n",
        "\n",
        "              src = torch.FloatTensor(img).unsqueeze(0).to(DEVICE)\n",
        "              if CHANNELS == 1:\n",
        "                src = transforms.Grayscale(CHANNELS)(src)\n",
        "              out_indexes = model.predict(src)\n",
        "              pred = indicies_to_text(out_indexes[0], idx2char)\n",
        "              preds[filename] = pred\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __init__(self, X_type=None, Y_type=None):\n",
        "        self.X_type = X_type\n",
        "\n",
        "    def __call__(self, X):\n",
        "        X = X.transpose((2, 0, 1))\n",
        "        X = torch.from_numpy(X)\n",
        "        if self.X_type is not None:\n",
        "            X = X.type(self.X_type)\n",
        "        return X\n",
        "\n",
        "\n",
        "def log_config(model):\n",
        "    print('transformer layers: {}'.format(model.enc_layers))\n",
        "    print('transformer heads: {}'.format(model.transformer.nhead))\n",
        "    print('hidden dim: {}'.format(model.decoder.embedding_dim))\n",
        "    print('num classes: {}'.format(model.decoder.num_embeddings))\n",
        "    print('backbone: {}'.format(model.backbone_name))\n",
        "    print('dropout: {}'.format(model.pos_encoder.dropout.p))\n",
        "    print(f'{count_parameters(model):,} trainable parameters')\n",
        "\n",
        "\n",
        "def log_metrics(metrics, path_to_logs=None):\n",
        "    if path_to_logs != None:\n",
        "      f = open(path_to_logs, 'a')\n",
        "    if metrics['epoch'] == 1:\n",
        "      if path_to_logs != None:\n",
        "        f.write('Epoch\\tTrain_loss\\tValid_loss\\tCER\\tWER\\tTime\\n')\n",
        "      print('Epoch   Train_loss   Valid_loss   CER   WER    Time    LR')\n",
        "      print('-----   -----------  ----------   ---   ---    ----    ---')\n",
        "    print('{:02d}       {:.2f}         {:.2f}       {:.2f}   {:.2f}   {:.2f}   {:.7f}'.format(\\\n",
        "        metrics['epoch'], metrics['train_loss'], metrics['loss'], metrics['cer'], \\\n",
        "        metrics['wer'], metrics['time'], metrics['lr']))\n",
        "    if path_to_logs != None:\n",
        "      f.write(str(metrics['epoch'])+'\\t'+str(metrics['train_loss'])+'\\t'+str(metrics['loss'])+'\\t'+str(metrics['cer'])+'\\t'+str(metrics['wer'])+'\\t'+str(metrics['time'])+'\\n')\n",
        "      f.close()\n",
        "\n",
        "def main(path,det_model,rec_model):\n",
        "  print(path)\n",
        "  array_of_boxes,sign = detect(path,det_model)\n",
        "  print(array_of_boxes)\n",
        "  image = Image.open(path)\n",
        "  ans = extract_rectangles(image, array_of_boxes)\n",
        "  name_image = path[path.rfind('/')+1:]\n",
        "  directory = './parts of ' + name_image + '/'\n",
        "\n",
        "  try:\n",
        "      os.mkdir(directory)\n",
        "      print(\"Папка успешно создана.\")\n",
        "  except FileExistsError:\n",
        "      print(\"Папка уже существует.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Произошла ошибка: {e}\")\n",
        "\n",
        "  for i in ans:\n",
        "    i.save(directory+ str(i) + \".png\")\n",
        "\n",
        "  preds = prediction(rec_model, directory, char2idx, idx2char)\n",
        "\n",
        "  labels = []\n",
        "\n",
        "  for i in preds.values():\n",
        "    labels.append(i)\n",
        "\n",
        "  print(len(sign), len(labels),len(array_of_boxes))\n",
        "\n",
        "  df = pd.DataFrame({\"coordinates\":array_of_boxes,\"content\":labels,\"signature\":sign})\n",
        "  df[\"signature\"] = df[\"signature\"].astype(bool)\n",
        "  df.to_json(\"./temp/\"+name_image[:name_image.find('.')]+ \".json\")\n",
        "  df.to_csv(\"./temp/\"+name_image[:name_image.find('.')]+ \".csv\")\n",
        "  clear(directory)\n",
        "  os.rmdir(directory)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.enc_layers = enc_layers\n",
        "        self.dec_layers = dec_layers\n",
        "        self.backbone_name = 'conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)'\n",
        "\n",
        "        self.conv0 = Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv1 = Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv2 = Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n",
        "        self.conv3 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv4 = Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n",
        "        self.conv5 = Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv6 = Conv2d(512, 512, kernel_size=(2, 1), stride=(1, 1))\n",
        "\n",
        "        self.pool1 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        self.pool3 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        self.pool5 = MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
        "\n",
        "        self.bn0 = BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn1 = BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn2 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn3 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn4 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn5 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn6 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "        self.activ = LeakyReLU()\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(hidden, dropout)\n",
        "        self.decoder = nn.Embedding(outtoken, hidden)\n",
        "        self.pos_decoder = PositionalEncoding(hidden, dropout)\n",
        "        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n",
        "                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden, outtoken)\n",
        "        self.src_mask = None\n",
        "        self.trg_mask = None\n",
        "        self.memory_mask = None\n",
        "\n",
        "        log_config(self)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = torch.triu(torch.ones(sz, sz, device=DEVICE), 1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        return mask\n",
        "\n",
        "    def make_len_mask(self, inp):\n",
        "        return (inp == 0).transpose(0, 1)\n",
        "\n",
        "    def _get_features(self, src):\n",
        "        '''\n",
        "        params\n",
        "        ---\n",
        "        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
        "            B - batch, C - channel, H - height, W - width\n",
        "        returns\n",
        "        ---\n",
        "        x : Tensor : [W,B,CH]\n",
        "        '''\n",
        "        x = self.activ(self.bn0(self.conv0(src)))\n",
        "        x = self.pool1(self.activ(self.bn1(self.conv1(x))))\n",
        "        x = self.activ(self.bn2(self.conv2(x)))\n",
        "        x = self.pool3(self.activ(self.bn3(self.conv3(x))))\n",
        "        x = self.activ(self.bn4(self.conv4(x)))\n",
        "        x = self.pool5(self.activ(self.bn5(self.conv5(x))))\n",
        "        x = self.activ(self.bn6(self.conv6(x)))\n",
        "        x = x.permute(0, 3, 1, 2).flatten(2).permute(1, 0, 2)\n",
        "        return x\n",
        "\n",
        "    def predict(self, batch):\n",
        "        '''\n",
        "        params\n",
        "        ---\n",
        "        batch : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
        "            B - batch, C - channel, H - height, W - width\n",
        "\n",
        "        returns\n",
        "        ---\n",
        "        result : List [64, -1] : [B, -1]\n",
        "            preticted sequences of tokens' indexes\n",
        "        '''\n",
        "        result = []\n",
        "        for item in batch:\n",
        "          x = self._get_features(item.unsqueeze(0))\n",
        "          memory = self.transformer.encoder(self.pos_encoder(x))\n",
        "          out_indexes = [ALPHABET.index('SOS'), ]\n",
        "          for i in range(100):\n",
        "              trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(DEVICE)\n",
        "              output = self.fc_out(self.transformer.decoder(self.pos_decoder(self.decoder(trg_tensor)), memory))\n",
        "\n",
        "              out_token = output.argmax(2)[-1].item()\n",
        "              out_indexes.append(out_token)\n",
        "              if out_token == ALPHABET.index('EOS'):\n",
        "                  break\n",
        "          result.append(out_indexes)\n",
        "        return result\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        '''\n",
        "        params\n",
        "        ---\n",
        "        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
        "            B - batch, C - channel, H - height, W - width\n",
        "        trg : Tensor [13, 64] : [L,B]\n",
        "            L - max length of label\n",
        "        '''\n",
        "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
        "            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n",
        "\n",
        "        x = self._get_features(src)\n",
        "        src_pad_mask = self.make_len_mask(x[:, :, 0])\n",
        "        src = self.pos_encoder(x)\n",
        "        trg_pad_mask = self.make_len_mask(trg)\n",
        "        trg = self.decoder(trg)\n",
        "        trg = self.pos_decoder(trg)\n",
        "\n",
        "        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n",
        "                                  memory_mask=self.memory_mask,\n",
        "                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n",
        "                                  memory_key_padding_mask=src_pad_mask)\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "MODEL = 'model2'\n",
        "HIDDEN = 512\n",
        "ENC_LAYERS = 2\n",
        "DEC_LAYERS = 2\n",
        "N_HEADS = 4\n",
        "LENGTH = 42\n",
        "ALPHABET = ['PAD', 'SOS', ' ', '!', '\"', '%', '(', ')', ',', '-', '.', '/',\n",
        "            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?',\n",
        "            '[', ']', '«', '»', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И',\n",
        "            'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х',\n",
        "            'Ц', 'Ч', 'Ш', 'Щ', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е',\n",
        "            'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т',\n",
        "            'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я',\n",
        "            'ё', 'EOS']\n",
        "\n",
        "### TRAINING ###\n",
        "BATCH_SIZE = 16\n",
        "DROPOUT = 0.2\n",
        "N_EPOCHS = 10\n",
        "CHECKPOINT_FREQ = 10 # save checkpoint every 10 epochs\n",
        "DEVICE = 'cpu' # or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "RANDOM_SEED = 42\n",
        "SCHUDULER_ON = True # \"ReduceLROnPlateau\"\n",
        "PATIENCE = 5 # for ReduceLROnPlateau\n",
        "OPTIMIZER_NAME = 'Adam' # or \"SGD\"\n",
        "LR = 2e-6\n",
        "\n",
        "### TESTING ###\n",
        "CASE = False # is case taken into account or not while evaluating\n",
        "PUNCT = False # are punctuation marks taken into account\n",
        "\n",
        "### INPUT IMAGE PARAMETERS ###\n",
        "WIDTH = 256\n",
        "HEIGHT = 64\n",
        "CHANNELS = 1 # 3\n",
        "\n",
        "\n",
        "char2idx = {char: idx for idx, char in enumerate(ALPHABET)}\n",
        "idx2char = {idx: char for idx, char in enumerate(ALPHABET)}\n",
        "\n",
        "rec_model = TransformerModel(len(ALPHABET), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,\n",
        "                          nhead=N_HEADS, dropout=DROPOUT).to(DEVICE)\n",
        "rec_model.load_state_dict(torch.load('./model.pt',map_location = torch.device('cpu')))\n",
        "\n",
        "\n",
        "#f = open(\"./temp/image_name.txt\",'r')\n",
        "print(\"Enter path\")\n",
        "path = input()\n",
        "main(path,det_model,rec_model)\n",
        "#f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clear(\"parts of 1_pass_1.png/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ToPB_bT0u4D",
        "outputId": "69e94056-670e-4c67-c865-679c8bd443d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл <PIL.Image.Image image mode=RGBA size=106x42 at 0x7879C5EDEB90>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=388x52 at 0x7879C62E2890>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=268x52 at 0x7879C5EDC970>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=242x66 at 0x7879C62E3D30>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=210x64 at 0x7879C5EDC190>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=314x50 at 0x7879C62E3520>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=240x54 at 0x7879C5EDCA60>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=310x50 at 0x7879C5EDE2C0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=196x40 at 0x7879C5EDE080>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=320x48 at 0x7879C62E2D10>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=374x70 at 0x7879C5EDD540>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=114x48 at 0x7879C5EDD480>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=238x60 at 0x7878FA8A0D00>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=242x40 at 0x7879C5EDC130>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=244x64 at 0x7878F4E774C0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=182x112 at 0x7879C5EDE050>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=206x44 at 0x7879C5EDE5C0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=214x64 at 0x7879C5EDC880>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=156x58 at 0x7879C5EDDC60>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=248x66 at 0x7879C5EDE260>.png удалён.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErtZW8kkpx9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}