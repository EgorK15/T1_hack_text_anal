{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "41nr6QimSw7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.39.0 Augmentor editdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r7NSt50hak-",
        "outputId": "7f647159-b8a5-49fa-8f2e-4213fb9123b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.39.0\n",
            "  Downloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Augmentor\n",
            "  Downloading Augmentor-0.2.12-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.0)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0) (4.66.5)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from Augmentor) (10.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0) (2024.8.30)\n",
            "Downloading transformers-4.39.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Augmentor-0.2.12-py2.py3-none-any.whl (38 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytesseract, Augmentor, tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed Augmentor-0.2.12 pytesseract-0.3.13 tokenizers-0.15.2 transformers-4.39.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==2.5 ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPoG8hnlZoiv",
        "outputId": "a05db41e-7fbb-4c34-81dd-98fb1d81f0fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.5 in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.23-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Downloading ultralytics-8.3.23-py3-none-any.whl (877 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.6/877.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.23 ultralytics-thop-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загружаем библиотеки"
      ],
      "metadata": {
        "id": "vbJhejXNaSKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vR8FIAzLaQm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import pandas as pd\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import Augmentor\n",
        "from torch import nn\n",
        "from torch.nn import Conv2d, MaxPool2d, BatchNorm2d, LeakyReLU\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import editdistance\n",
        "from tqdm import tqdm\n",
        "from time import time\n",
        "import random\n",
        "import string\n",
        "import math\n",
        "from collections import Counter\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "i1UT__edaBt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8719ff50-3487-45ab-fb0e-ff1af0dce016"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.20 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Предобработка изображений"
      ],
      "metadata": {
        "id": "XhVnHNmtaIqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Детекция слов. Возвращаем картинки слов"
      ],
      "metadata": {
        "id": "ajSGsntDabRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLOv8 для детекции (пробный вариант)"
      ],
      "metadata": {
        "id": "HoNKZ0GZ80T0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделение по детектингу"
      ],
      "metadata": {
        "id": "TsT7JsbhYEqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_rectangles(image, rectangles):\n",
        "    extracted_images = []\n",
        "\n",
        "    for  har in rectangles:\n",
        "        xc = har[0]\n",
        "        yc = har[1]\n",
        "        w = har[2]\n",
        "        h = har[3]\n",
        "        x1 = xc - w// 2\n",
        "        y1 = yc - h// 2\n",
        "        x2 = xc + w // 2\n",
        "        y2 = yc + h // 2\n",
        "\n",
        "        cropped_img = image.crop((x1, y1, x2, y2))\n",
        "        cropped_img.show()\n",
        "        extracted_images.append(cropped_img)\n",
        "\n",
        "    return extracted_images\n"
      ],
      "metadata": {
        "id": "jMQAB1jBYItO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the weights from our repository\n",
        "\n",
        "def detect(path,model):\n",
        "  res = det_model.predict(source=path, project='.',name='detected', exist_ok=True, save=True, show=False, show_labels=False, show_conf=False, conf=0.5)\n",
        "\n",
        "  image = Image.open(path)\n",
        "\n",
        "  array_of_boxes = []\n",
        "  for b in res[0].boxes.xywh:\n",
        "    array_of_boxes.append(b.tolist())\n",
        "  sign = res[0].boxes.cls.tolist()\n",
        "  return array_of_boxes,sign\n",
        "\n",
        "det_model = YOLO(\"best (1).pt\")\n",
        "res = det_model.predict(source=\"1_pass_1.png\", project='.',name='detected', exist_ok=True, save=True, show=True, show_labels=True, show_conf=False, conf=0.5)\n",
        "\n",
        "\n",
        "print(detect(path,det_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUL5tsur9y5G",
        "outputId": "da092317-a15f-48ac-e574-ffbbd470563a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "\n",
            "image 1/1 /content/1_pass_1.png: 640x480 19 not_signs, 1 sign, 150.5ms\n",
            "Speed: 3.4ms preprocess, 150.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mdetected\u001b[0m\n",
            "\n",
            "image 1/1 /content/1_pass_1.png: 640x480 19 not_signs, 1 sign, 143.6ms\n",
            "Speed: 3.4ms preprocess, 143.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mdetected\u001b[0m\n",
            "([[564.6967163085938, 1045.0654296875, 210.4940185546875, 64.945068359375], [742.272216796875, 187.37490844726562, 107.25115966796875, 43.45683288574219], [639.8463134765625, 1041.1761474609375, 375.00360107421875, 70.640380859375], [406.02032470703125, 999.91650390625, 115.267822265625, 48.26531982421875], [576.8048095703125, 765.8109130859375, 249.61666870117188, 66.5152587890625], [183.2305450439453, 446.08441162109375, 183.80892944335938, 113.84066772460938], [543.5894775390625, 882.1089477539062, 240.73626708984375, 55.653564453125], [267.8492736816406, 194.3089599609375, 215.689453125, 64.29965209960938], [518.5321655273438, 195.34548950195312, 238.0396728515625, 60.63056945800781], [309.5284118652344, 242.46510314941406, 314.5657958984375, 50.470611572265625], [665.92724609375, 242.24551391601562, 243.344482421875, 41.83235168457031], [700.2841186523438, 292.3075256347656, 206.8282470703125, 44.94488525390625], [394.9449462890625, 154.95631408691406, 242.5352783203125, 66.18209075927734], [589.929931640625, 933.2787475585938, 388.7536315917969, 52.8656005859375], [508.19024658203125, 1088.6424560546875, 321.0772705078125, 48.43603515625], [256.26385498046875, 299.1544189453125, 197.3598175048828, 40.612060546875], [716.9022216796875, 985.1146240234375, 269.57843017578125, 53.25262451171875], [686.8961181640625, 503.74176025390625, 157.06201171875, 59.2840576171875], [710.1586303710938, 141.34036254882812, 310.881103515625, 51.94984436035156], [703.5232543945312, 1044.32763671875, 245.160400390625, 64.4019775390625]], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(detect(path,det_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjC_Whg-5hqJ",
        "outputId": "554d2b54-735c-4663-c0f2-3574913ea3e6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/2_p_17.png: 480x640 2 words, 160.9ms\n",
            "Speed: 4.8ms preprocess, 160.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1mdetected\u001b[0m\n",
            "[[458.2481689453125, 195.3785400390625, 68.67172241210938, 14.799942016601562], [411.49530029296875, 126.51167297363281, 29.017303466796875, 11.491127014160156]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans = extract_rectangles(image, array_of_boxes)"
      ],
      "metadata": {
        "id": "yg1viGDHP6Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory = 'photos/'\n",
        "\n",
        "try:\n",
        "    os.mkdir(directory)\n",
        "    print(\"Папка успешно создана.\")\n",
        "except FileExistsError:\n",
        "    print(\"Папка уже существует.\")\n",
        "except Exception as e:\n",
        "    print(f\"Произошла ошибка: {e}\")\n",
        "\n",
        "for i in ans:\n",
        "   i.save(directory+ str(i) + \".png\")"
      ],
      "metadata": {
        "id": "8mAcdzX61Ipb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8f5270-7971-47e5-f630-d2cc5fc6f7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Папка успешно создана.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Модели"
      ],
      "metadata": {
        "id": "oR8RAE-GqIQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN + трансформер"
      ],
      "metadata": {
        "id": "4_Bi_HkOn6Vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Директории"
      ],
      "metadata": {
        "id": "jFOdOaGDVrgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = './' # work directory\n",
        "PATH_TEST_DIR = 'test/'\n",
        "PATH_TEST_LABELS =  'test.tsv'\n",
        "PATH_TRAIN_DIR =  'train/'\n",
        "PATH_TRAIN_LABELS =  'train.tsv'\n",
        "PREDICT_PATH = \"photos/\"\n",
        "CHECKPOINT_PATH = DIR\n",
        "WEIGHTS_PATH =  \"./model.pt\"\n",
        "PATH_TEST_RESULTS = DIR+'/test_result.tsv'\n",
        "TRAIN_LOG = DIR+'train_log.tsv'"
      ],
      "metadata": {
        "id": "GNhQeBtHxBp8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Константы"
      ],
      "metadata": {
        "id": "pYfkj8lgVujY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'model2'\n",
        "HIDDEN = 512\n",
        "ENC_LAYERS = 2\n",
        "DEC_LAYERS = 2\n",
        "N_HEADS = 4\n",
        "LENGTH = 42\n",
        "ALPHABET = ['PAD', 'SOS', ' ', '!', '\"', '%', '(', ')', ',', '-', '.', '/',\n",
        "            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?',\n",
        "            '[', ']', '«', '»', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И',\n",
        "            'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х',\n",
        "            'Ц', 'Ч', 'Ш', 'Щ', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е',\n",
        "            'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т',\n",
        "            'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я',\n",
        "            'ё', 'EOS']\n",
        "\n",
        "### TRAINING ###\n",
        "BATCH_SIZE = 16\n",
        "DROPOUT = 0.2\n",
        "N_EPOCHS = 10\n",
        "CHECKPOINT_FREQ = 10 # save checkpoint every 10 epochs\n",
        "DEVICE = 'cpu' # or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "RANDOM_SEED = 42\n",
        "SCHUDULER_ON = True # \"ReduceLROnPlateau\"\n",
        "PATIENCE = 5 # for ReduceLROnPlateau\n",
        "OPTIMIZER_NAME = 'Adam' # or \"SGD\"\n",
        "LR = 2e-6\n",
        "\n",
        "### TESTING ###\n",
        "CASE = False # is case taken into account or not while evaluating\n",
        "PUNCT = False # are punctuation marks taken into account\n",
        "\n",
        "### INPUT IMAGE PARAMETERS ###\n",
        "WIDTH = 256\n",
        "HEIGHT = 64\n",
        "CHANNELS = 1 # 3"
      ],
      "metadata": {
        "id": "eakVIWLouxMh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение"
      ],
      "metadata": {
        "id": "g74jayF2WV_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.scale = torch.nn.Parameter(torch.ones(1))\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(\n",
        "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.scale * self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "# convert images and labels into defined data structures\n",
        "def process_data(image_dir, labels_dir, ignore=[]):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    image_dir : str\n",
        "      path to directory with images\n",
        "    labels_dir : str\n",
        "      path to tsv file with labels\n",
        "    returns\n",
        "    ---\n",
        "    img2label : dict\n",
        "      keys are names of images and values are correspondent labels\n",
        "    chars : list\n",
        "      all unique chars used in data\n",
        "    all_labels : list\n",
        "    \"\"\"\n",
        "\n",
        "    chars = []\n",
        "    img2label = dict()\n",
        "\n",
        "    raw = open(labels_dir, 'r', encoding='utf-8').read()\n",
        "    temp = raw.split('\\n')\n",
        "    for t in temp:\n",
        "        try:\n",
        "            x = t.split('\\t')\n",
        "            flag = False\n",
        "            for item in ignore:\n",
        "                if item in x[1]:\n",
        "                    flag = True\n",
        "            if flag == False:\n",
        "                img2label[image_dir + x[0]] = x[1]\n",
        "                for char in x[1]:\n",
        "                    if char not in chars:\n",
        "                        chars.append(char)\n",
        "        except:\n",
        "            print('ValueError:', x)\n",
        "            pass\n",
        "\n",
        "    all_labels = sorted(list(set(list(img2label.values()))))\n",
        "    chars.sort()\n",
        "    chars = ['PAD', 'SOS'] + chars + ['EOS']\n",
        "\n",
        "    return img2label, chars, all_labels\n",
        "\n",
        "\n",
        "# TRANSLATE INDICIES TO TEXT\n",
        "def indicies_to_text(indexes, idx2char):\n",
        "    text = \"\".join([idx2char[i] for i in indexes])\n",
        "    text = text.replace('EOS', '').replace('PAD', '').replace('SOS', '')\n",
        "    return text\n",
        "\n",
        "\n",
        "# COMPUTE CHARACTER ERROR RATE\n",
        "def char_error_rate(p_seq1, p_seq2):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    p_seq1 : str\n",
        "    p_seq2 : str\n",
        "    returns\n",
        "    ---\n",
        "    cer : float\n",
        "    \"\"\"\n",
        "    p_vocab = set(p_seq1 + p_seq2)\n",
        "    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n",
        "    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n",
        "    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n",
        "    return editdistance.eval(''.join(c_seq1),\n",
        "                             ''.join(c_seq2)) / max(len(c_seq1), len(c_seq2))\n",
        "\n",
        "\n",
        "# RESIZE AND NORMALIZE IMAGE\n",
        "def process_image(img):\n",
        "    \"\"\"\n",
        "    params:\n",
        "    ---\n",
        "    img : np.array\n",
        "    returns\n",
        "    ---\n",
        "    img : np.array\n",
        "    \"\"\"\n",
        "    w, h, _ = img.shape\n",
        "    new_w = HEIGHT\n",
        "    new_h = int(h * (new_w / w))\n",
        "    img = cv2.resize(img, (new_h, new_w))\n",
        "    w, h, _ = img.shape\n",
        "\n",
        "    img = img.astype('float32')\n",
        "\n",
        "    new_h = WIDTH\n",
        "    if h < new_h:\n",
        "        add_zeros = np.full((w, new_h - h, 3), 255)\n",
        "        img = np.concatenate((img, add_zeros), axis=1)\n",
        "\n",
        "    if h > new_h:\n",
        "        img = cv2.resize(img, (new_h, new_w))\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# GENERATE IMAGES FROM FOLDER\n",
        "def generate_data(img_paths):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    names : list of str\n",
        "        paths to images\n",
        "    returns\n",
        "    ---\n",
        "    data_images : list of np.array\n",
        "        images in np.array format\n",
        "    \"\"\"\n",
        "    data_images = []\n",
        "    for path in tqdm(img_paths):\n",
        "        img = np.asarray(Image.open(path).convert('RGB'))\n",
        "        try:\n",
        "            img = process_image(img)\n",
        "            data_images.append(img.astype('uint8'))\n",
        "        except:\n",
        "            print(path)\n",
        "            img = process_image(img)\n",
        "    return data_images\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, loader, case=True, punct=True):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    model : nn.Module\n",
        "    criterion : nn.Object\n",
        "    loader : torch.utils.data.DataLoader\n",
        "\n",
        "    returns\n",
        "    ---\n",
        "    epoch_loss / len(loader) : float\n",
        "        overall loss\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    metrics = {'loss': 0, 'wer': 0, 'cer': 0}\n",
        "    result = {'true': [], 'predicted': [], 'wer': []}\n",
        "    with torch.no_grad():\n",
        "        for (src, trg) in loader:\n",
        "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "            logits = model(src, trg[:-1, :])\n",
        "            loss = criterion(logits.view(-1, logits.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n",
        "            out_indexes = model.predict(src)\n",
        "\n",
        "            true_phrases = [indicies_to_text(trg.T[i][1:], ALPHABET) for i in range(BATCH_SIZE)]\n",
        "            pred_phrases = [indicies_to_text(out_indexes[i], ALPHABET) for i in range(BATCH_SIZE)]\n",
        "\n",
        "            if not case:\n",
        "                true_phrases = [phrase.lower() for phrase in true_phrases]\n",
        "                pred_phrases = [phrase.lower() for phrase in pred_phrases]\n",
        "            if not punct:\n",
        "                true_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n",
        "                                for phrase in true_phrases]\n",
        "                pred_phrases = [phrase.translate(str.maketrans('', '', string.punctuation))\\\n",
        "                                for phrase in pred_phrases]\n",
        "\n",
        "            metrics['loss'] += loss.item()\n",
        "            metrics['cer'] += sum([char_error_rate(true_phrases[i], pred_phrases[i]) \\\n",
        "                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n",
        "            metrics['wer'] += sum([int(true_phrases[i] != pred_phrases[i]) \\\n",
        "                        for i in range(BATCH_SIZE)])/BATCH_SIZE\n",
        "\n",
        "            for i in range(len(true_phrases)):\n",
        "              result['true'].append(true_phrases[i])\n",
        "              result['predicted'].append(pred_phrases[i])\n",
        "              result['wer'].append(char_error_rate(true_phrases[i], pred_phrases[i]))\n",
        "\n",
        "    for key in metrics.keys():\n",
        "      metrics[key] /= len(loader)\n",
        "\n",
        "    return metrics, result\n",
        "\n",
        "\n",
        "# MAKE PREDICTION\n",
        "def prediction(model, test_dir, char2idx, idx2char):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    model : nn.Module\n",
        "    test_dir : str\n",
        "        path to directory with images\n",
        "    char2idx : dict\n",
        "        map from chars to indicies\n",
        "    id2char : dict\n",
        "        map from indicies to chars\n",
        "\n",
        "    returns\n",
        "    ---\n",
        "    preds : dict\n",
        "        key : name of image in directory\n",
        "        value : dict with keys ['p_value', 'predicted_label']\n",
        "    \"\"\"\n",
        "    preds = {}\n",
        "    os.makedirs('/output', exist_ok=True)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for filename in os.listdir(test_dir):\n",
        "            file_path = os.path.join(test_dir, filename)\n",
        "            if (os.path.isfile(file_path)):\n",
        "              img = Image.open(test_dir + filename).convert('RGB')\n",
        "\n",
        "              img = process_image(np.asarray(img)).astype('uint8')\n",
        "              img = img / img.max()\n",
        "              img = np.transpose(img, (2, 0, 1))\n",
        "\n",
        "              src = torch.FloatTensor(img).unsqueeze(0).to(DEVICE)\n",
        "              if CHANNELS == 1:\n",
        "                src = transforms.Grayscale(CHANNELS)(src)\n",
        "              out_indexes = model.predict(src)\n",
        "              pred = indicies_to_text(out_indexes[0], idx2char)\n",
        "              preds[filename] = pred\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __init__(self, X_type=None, Y_type=None):\n",
        "        self.X_type = X_type\n",
        "\n",
        "    def __call__(self, X):\n",
        "        X = X.transpose((2, 0, 1))\n",
        "        X = torch.from_numpy(X)\n",
        "        if self.X_type is not None:\n",
        "            X = X.type(self.X_type)\n",
        "        return X\n",
        "\n",
        "\n",
        "def log_config(model):\n",
        "    print('transformer layers: {}'.format(model.enc_layers))\n",
        "    print('transformer heads: {}'.format(model.transformer.nhead))\n",
        "    print('hidden dim: {}'.format(model.decoder.embedding_dim))\n",
        "    print('num classes: {}'.format(model.decoder.num_embeddings))\n",
        "    print('backbone: {}'.format(model.backbone_name))\n",
        "    print('dropout: {}'.format(model.pos_encoder.dropout.p))\n",
        "    print(f'{count_parameters(model):,} trainable parameters')\n",
        "\n",
        "\n",
        "def log_metrics(metrics, path_to_logs=None):\n",
        "    if path_to_logs != None:\n",
        "      f = open(path_to_logs, 'a')\n",
        "    if metrics['epoch'] == 1:\n",
        "      if path_to_logs != None:\n",
        "        f.write('Epoch\\tTrain_loss\\tValid_loss\\tCER\\tWER\\tTime\\n')\n",
        "      print('Epoch   Train_loss   Valid_loss   CER   WER    Time    LR')\n",
        "      print('-----   -----------  ----------   ---   ---    ----    ---')\n",
        "    print('{:02d}       {:.2f}         {:.2f}       {:.2f}   {:.2f}   {:.2f}   {:.7f}'.format(\\\n",
        "        metrics['epoch'], metrics['train_loss'], metrics['loss'], metrics['cer'], \\\n",
        "        metrics['wer'], metrics['time'], metrics['lr']))\n",
        "    if path_to_logs != None:\n",
        "      f.write(str(metrics['epoch'])+'\\t'+str(metrics['train_loss'])+'\\t'+str(metrics['loss'])+'\\t'+str(metrics['cer'])+'\\t'+str(metrics['wer'])+'\\t'+str(metrics['time'])+'\\n')\n",
        "      f.close()\n",
        "\n",
        "\n",
        "# plot images\n",
        "def show_img_grid(images, labels, N):\n",
        "    n = int(N**(0.5))\n",
        "    k = 0\n",
        "    f, axarr = plt.subplots(n,n,figsize=(10,10))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            axarr[i,j].set_title(labels[k])\n",
        "            axarr[i,j].imshow(images[k])\n",
        "            k += 1\n"
      ],
      "metadata": {
        "id": "uf5WSU0SvmW3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_labels(s, char2idx):\n",
        "    return [char2idx['SOS']] + [char2idx[i] for i in s if i in char2idx.keys()] + [char2idx['EOS']]\n",
        "\n",
        "# store list of images' names (in directory) and does some operations with images\n",
        "class TextLoader(torch.utils.data.Dataset):\n",
        "    def __init__(self, images_name, labels, transforms, char2idx, idx2char):\n",
        "        \"\"\"\n",
        "        params\n",
        "        ---\n",
        "        images_name : list\n",
        "            list of names of images (paths to images)\n",
        "        labels : list\n",
        "            list of labels to correspondent images from images_name list\n",
        "        char2idx : dict\n",
        "        idx2char : dict\n",
        "        \"\"\"\n",
        "        self.images_name = images_name\n",
        "        self.labels = labels\n",
        "        self.char2idx = char2idx\n",
        "        self.idx2char = idx2char\n",
        "        self.transform = transforms\n",
        "\n",
        "    def _transform(self, X):\n",
        "        j = np.random.randint(0, 3, 1)[0]\n",
        "        if j == 0:\n",
        "            return self.transform(X)\n",
        "        if j == 1:\n",
        "            return tt(ld(vignet(X)))\n",
        "        if j == 2:\n",
        "            return tt(ld(un(X)))\n",
        "\n",
        "\n",
        "    # shows some stats about dataset\n",
        "    def get_info(self):\n",
        "        N = len(self.labels)\n",
        "        max_len = -1\n",
        "        for label in self.labels:\n",
        "            if len(label) > max_len:\n",
        "                max_len = len(label)\n",
        "        counter = Counter(''.join(self.labels))\n",
        "        counter = dict(sorted(counter.items(), key=lambda item: item[1]))\n",
        "        print(\n",
        "            'Size of dataset: {}\\nMax length of expression: {}\\nThe most common char: {}\\nThe least common char: {}'.format( \\\n",
        "                N, max_len, list(counter.items())[-1], list(counter.items())[0]))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.images_name[index]\n",
        "        img = self.transform(img)\n",
        "        img = img / img.max()\n",
        "        img = img ** (random.random() * 0.7 + 0.6)\n",
        "\n",
        "        label = text_to_labels(self.labels[index], self.char2idx)\n",
        "        return (torch.FloatTensor(img), torch.LongTensor(label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "# MAKE TEXT TO BE THE SAME LENGTH\n",
        "class TextCollate():\n",
        "    def __call__(self, batch):\n",
        "        x_padded = []\n",
        "        y_padded = torch.LongTensor(LENGTH, len(batch))\n",
        "        y_padded.zero_()\n",
        "\n",
        "        for i in range(len(batch)):\n",
        "            x_padded.append(batch[i][0].unsqueeze(0))\n",
        "            y = batch[i][1]\n",
        "            y_padded[:y.size(0), i] = y\n",
        "\n",
        "        x_padded = torch.cat(x_padded)\n",
        "        return x_padded, y_padded\n",
        "\n",
        "\n",
        "p = Augmentor.Pipeline()\n",
        "p.shear(max_shear_left=2, max_shear_right=2, probability=0.7)\n",
        "p.random_distortion(probability=1.0, grid_width=3, grid_height=3, magnitude=11)\n",
        "\n",
        "TRAIN_TRANSFORMS = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Grayscale(CHANNELS),\n",
        "            p.torch_transform(),  # random distortion and shear\n",
        "            transforms.ColorJitter(contrast=(0.5,1),saturation=(0.5,1)),\n",
        "            transforms.RandomRotation(degrees=(-9, 9)),\n",
        "            transforms.RandomAffine(10, None, [0.6 ,1] ,3 ,fill = 255),\n",
        "            #transforms.transforms.GaussianBlur(3, sigma=(0.1, 1.9)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "TEST_TRANSFORMS = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Grayscale(CHANNELS),\n",
        "            transforms.ToTensor()\n",
        "        ])"
      ],
      "metadata": {
        "id": "wqSEIeuuv1hS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель"
      ],
      "metadata": {
        "id": "FqSFMnZqZnXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.enc_layers = enc_layers\n",
        "        self.dec_layers = dec_layers\n",
        "        self.backbone_name = 'conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)'\n",
        "\n",
        "        self.conv0 = Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv1 = Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv2 = Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n",
        "        self.conv3 = Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv4 = Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1))\n",
        "        self.conv5 = Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv6 = Conv2d(512, 512, kernel_size=(2, 1), stride=(1, 1))\n",
        "\n",
        "        self.pool1 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        self.pool3 = MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "        self.pool5 = MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
        "\n",
        "        self.bn0 = BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn1 = BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn2 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn3 = BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn4 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn5 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.bn6 = BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "        self.activ = LeakyReLU()\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(hidden, dropout)\n",
        "        self.decoder = nn.Embedding(outtoken, hidden)\n",
        "        self.pos_decoder = PositionalEncoding(hidden, dropout)\n",
        "        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n",
        "                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden, outtoken)\n",
        "        self.src_mask = None\n",
        "        self.trg_mask = None\n",
        "        self.memory_mask = None\n",
        "\n",
        "        log_config(self)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = torch.triu(torch.ones(sz, sz, device=DEVICE), 1)\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "        return mask\n",
        "\n",
        "    def make_len_mask(self, inp):\n",
        "        return (inp == 0).transpose(0, 1)\n",
        "\n",
        "    def _get_features(self, src):\n",
        "        '''\n",
        "        params\n",
        "        ---\n",
        "        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
        "            B - batch, C - channel, H - height, W - width\n",
        "        returns\n",
        "        ---\n",
        "        x : Tensor : [W,B,CH]\n",
        "        '''\n",
        "        x = self.activ(self.bn0(self.conv0(src)))\n",
        "        x = self.pool1(self.activ(self.bn1(self.conv1(x))))\n",
        "        x = self.activ(self.bn2(self.conv2(x)))\n",
        "        x = self.pool3(self.activ(self.bn3(self.conv3(x))))\n",
        "        x = self.activ(self.bn4(self.conv4(x)))\n",
        "        x = self.pool5(self.activ(self.bn5(self.conv5(x))))\n",
        "        x = self.activ(self.bn6(self.conv6(x)))\n",
        "        x = x.permute(0, 3, 1, 2).flatten(2).permute(1, 0, 2)\n",
        "        return x\n",
        "\n",
        "    def predict(self, batch):\n",
        "        '''\n",
        "        params\n",
        "        ---\n",
        "        batch : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
        "            B - batch, C - channel, H - height, W - width\n",
        "\n",
        "        returns\n",
        "        ---\n",
        "        result : List [64, -1] : [B, -1]\n",
        "            preticted sequences of tokens' indexes\n",
        "        '''\n",
        "        result = []\n",
        "        for item in batch:\n",
        "          x = self._get_features(item.unsqueeze(0))\n",
        "          memory = self.transformer.encoder(self.pos_encoder(x))\n",
        "          out_indexes = [ALPHABET.index('SOS'), ]\n",
        "          for i in range(100):\n",
        "              trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(DEVICE)\n",
        "              output = self.fc_out(self.transformer.decoder(self.pos_decoder(self.decoder(trg_tensor)), memory))\n",
        "\n",
        "              out_token = output.argmax(2)[-1].item()\n",
        "              out_indexes.append(out_token)\n",
        "              if out_token == ALPHABET.index('EOS'):\n",
        "                  break\n",
        "          result.append(out_indexes)\n",
        "        return result\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        '''\n",
        "        params\n",
        "        ---\n",
        "        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
        "            B - batch, C - channel, H - height, W - width\n",
        "        trg : Tensor [13, 64] : [L,B]\n",
        "            L - max length of label\n",
        "        '''\n",
        "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
        "            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n",
        "\n",
        "        x = self._get_features(src)\n",
        "        src_pad_mask = self.make_len_mask(x[:, :, 0])\n",
        "        src = self.pos_encoder(x)\n",
        "        trg_pad_mask = self.make_len_mask(trg)\n",
        "        trg = self.decoder(trg)\n",
        "        trg = self.pos_decoder(trg)\n",
        "\n",
        "        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n",
        "                                  memory_mask=self.memory_mask,\n",
        "                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n",
        "                                  memory_key_padding_mask=src_pad_mask)\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "TsaE5lixn6sD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение"
      ],
      "metadata": {
        "id": "XbsCRewhZql8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, train_loader):\n",
        "    \"\"\"\n",
        "    params\n",
        "    ---\n",
        "    model : nn.Module\n",
        "    optimizer : nn.Object\n",
        "    criterion : nn.Object\n",
        "    train_loader : torch.utils.data.DataLoader\n",
        "    returns\n",
        "    ---\n",
        "    epoch_loss / len(train_loader) : float\n",
        "        overall loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, trg in train_loader:\n",
        "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg[:-1, :])\n",
        "\n",
        "        loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(train_loader)\n",
        "\n",
        "\n",
        "# GENERAL FUNCTION FROM TRAINING AND VALIDATION\n",
        "def fit(model, optimizer, scheduler, criterion, train_loader, val_loader, start_epoch=0, end_epoch=24):\n",
        "    metrics = []\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "        epoch_metrics = {}\n",
        "        start_time = time()\n",
        "        train_loss = train(model, optimizer, criterion, train_loader)\n",
        "        end_time = time()\n",
        "        epoch_metrics, _ = evaluate(model, criterion, val_loader)\n",
        "        epoch_metrics['train_loss'] = train_loss\n",
        "        epoch_metrics['epoch'] = epoch\n",
        "        epoch_metrics['time'] = end_time - start_time\n",
        "        epoch_metrics['lr'] = optimizer.param_groups[0][\"lr\"]\n",
        "        metrics.append(epoch_metrics)\n",
        "        log_metrics(epoch_metrics, TRAIN_LOG)\n",
        "        if scheduler != None:\n",
        "            scheduler.step(epoch_metrics['loss'])\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "4tDEY7dUv3LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "\n",
        "char2idx = {char: idx for idx, char in enumerate(ALPHABET)}\n",
        "idx2char = {idx: char for idx, char in enumerate(ALPHABET)}"
      ],
      "metadata": {
        "id": "y_OqVWIx8Bow"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"loading dataset {PATH_TRAIN_DIR} ...\")\n",
        "img2label, _, all_words = process_data(PATH_TRAIN_DIR, PATH_TRAIN_LABELS)\n",
        "img_names, labels = list(img2label.keys()), list(img2label.values())\n",
        "X_train = generate_data(img_names)\n",
        "y_train = labels\n",
        "\n",
        "train_dataset = TextLoader(X_train, y_train, TRAIN_TRANSFORMS, char2idx, idx2char)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory=True,\n",
        "                                           drop_last=True, collate_fn=TextCollate())\n",
        "\n",
        "print(f\"loading dataset {PATH_TEST_DIR} ...\")\n",
        "img2label, _, all_words = process_data(PATH_TEST_DIR, PATH_TEST_LABELS)\n",
        "img_names, labels = list(img2label.keys()), list(img2label.values())\n",
        "X_test = generate_data(img_names)\n",
        "y_test = labels\n",
        "\n",
        "test_dataset = TextLoader(X_test, y_test, TEST_TRANSFORMS, char2idx ,idx2char)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory=True,\n",
        "                                           drop_last=True, collate_fn=TextCollate())\n",
        "\n",
        "print(\"TRAIN DATASET:\")\n",
        "train_dataset.get_info()\n",
        "print(\"\\nTEST DATASET:\")\n",
        "test_dataset.get_info()"
      ],
      "metadata": {
        "id": "huvPG2iPwMbB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "a97c6c90-e5f1-44b6-c6fb-d5c1d3b61901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dataset train/ ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.tsv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-c978db67b205>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading dataset {PATH_TRAIN_DIR} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH_TRAIN_LABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimg_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-d9e21ce14e68>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(image_dir, labels_dir, ignore)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mimg2label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rec_model = TransformerModel(len(ALPHABET), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,\n",
        "                          nhead=N_HEADS, dropout=DROPOUT).to(DEVICE)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=char2idx['PAD'])\n",
        "optimizer = torch.optim.__getattribute__(OPTIMIZER_NAME)(rec_model.parameters(), lr=LR)\n",
        "\n",
        "if SCHUDULER_ON:\n",
        "    scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=PATIENCE)\n",
        "else:\n",
        "    scheduler = None\n",
        "\n",
        "print(f'checkpoints are saved in {CHECKPOINT_PATH} every {CHECKPOINT_FREQ} epochs')\n",
        "for epoch in range(1, N_EPOCHS, CHECKPOINT_FREQ):\n",
        "    fit(rec_model, optimizer, scheduler, criterion, train_loader, test_loader, epoch, epoch+CHECKPOINT_FREQ)\n",
        "    torch.save(rec_model.state_dict(), CHECKPOINT_PATH+'checkpoint_{}.pt'.format(epoch+CHECKPOINT_FREQ))"
      ],
      "metadata": {
        "id": "Uh9EgnkewZ0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка модели"
      ],
      "metadata": {
        "id": "AsJeIyN4Z81C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rec_model = TransformerModel(len(ALPHABET), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS,\n",
        "                          nhead=N_HEADS, dropout=DROPOUT).to(DEVICE)\n",
        "rec_model.load_state_dict(torch.load('./model.pt',map_location = torch.device('cpu')))"
      ],
      "metadata": {
        "id": "TNsf8rAKu5dS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35751e18-1bc5-4308-eadf-cccd0ae1316b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer layers: 2\n",
            "transformer heads: 4\n",
            "hidden dim: 512\n",
            "num classes: 92\n",
            "backbone: conv(64)->conv(64)->conv(128)->conv(256)->conv(256)->conv(512)->conv(512)\n",
            "dropout: 0.2\n",
            "19,838,174 trainable parameters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''if os.path.isfile(os.path.join(PREDICT_PATH, \".tsv\")):\n",
        "    os.remove(PREDICT_PATH + '.tsv')\n",
        "'''"
      ],
      "metadata": {
        "id": "lf2eS0OcCqEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предсказание"
      ],
      "metadata": {
        "id": "4KkPmIO2cE6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = prediction(rec_model, PREDICT_PATH, char2idx, idx2char)\n",
        "labels = []\n",
        "\n",
        "for i in preds.values():\n",
        "  labels.append(i)\n",
        "print(labels)\n",
        "df = pd.DataFrame({\"coordinates\":array_of_boxes,\"text\":labels})\n",
        "df.to_json(\"answer.json\")\n",
        "f = open(DIR+'/predictions.tsv', 'w')\n",
        "f.write('filename\\tprediction\\n')\n",
        "for item in preds.items():\n",
        "    f.write(item[0]+'\\t'+item[1]+'\\n')\n",
        "f.close()\n",
        "print(f'predictions are saved in {DIR}predictions.tsv')"
      ],
      "metadata": {
        "id": "n7xOoU2nwwoJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "7aedf81a-2660-4459-af79-083774607e0c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'photos/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-c69ca66b2d31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPREDICT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-1bc6b5beb175>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(model, test_dir, char2idx, idx2char)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m               \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'photos/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для апи"
      ],
      "metadata": {
        "id": "jV3SG5IC20Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clear(directory):\n",
        "  for filename in os.listdir(directory):\n",
        "    file_path = os.path.join(directory, filename)\n",
        "    try:\n",
        "        # Если это файл, то удалить его\n",
        "        if os.path.isfile(file_path):\n",
        "            os.remove(file_path)\n",
        "            print(f\"Файл {filename} удалён.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Не удалось удалить {filename}: {e}\")"
      ],
      "metadata": {
        "id": "HWsFAXBGLPyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(path,det_model,rec_model):\n",
        "\n",
        "  array_of_boxes,sign = detect(path,det_model)\n",
        "  print(array_of_boxes)\n",
        "  image = Image.open(path)\n",
        "  ans = extract_rectangles(image, array_of_boxes)\n",
        "  name_image = path[path.rfind('/')+1:]\n",
        "  directory = './parts of ' + name_image + '/'\n",
        "\n",
        "  try:\n",
        "      os.mkdir(directory)\n",
        "      print(\"Папка успешно создана.\")\n",
        "  except FileExistsError:\n",
        "      print(\"Папка уже существует.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Произошла ошибка: {e}\")\n",
        "\n",
        "  for i in ans:\n",
        "    i.save(directory+ str(i) + \".png\")\n",
        "\n",
        "  preds = prediction(rec_model, directory, char2idx, idx2char)\n",
        "\n",
        "  labels = []\n",
        "\n",
        "  for i in preds.values():\n",
        "    labels.append(i)\n",
        "\n",
        "  print(len(sign), len(labels),len(array_of_boxes))\n",
        "\n",
        "  df = pd.DataFrame({\"coordinates\":array_of_boxes,\"content\":labels,\"signature\":sign})\n",
        "  df[\"signature\"] = df[\"signature\"].astype(bool)\n",
        "  df.to_json(\"answer \" +name_image+ \".json\")\n",
        "\n",
        "  clear(directory)\n"
      ],
      "metadata": {
        "id": "qbwPF3__x59K"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(path,det_model,rec_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-6u5CUc-2_y",
        "outputId": "bdc2c6a8-c3b2-45cf-f5bf-1fe85b9597a6"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/1_pass_1.png: 640x480 19 not_signs, 1 sign, 143.6ms\n",
            "Speed: 4.7ms preprocess, 143.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "Results saved to \u001b[1mdetected\u001b[0m\n",
            "[[564.6967163085938, 1045.0654296875, 210.4940185546875, 64.945068359375], [742.272216796875, 187.37490844726562, 107.25115966796875, 43.45683288574219], [639.8463134765625, 1041.1761474609375, 375.00360107421875, 70.640380859375], [406.02032470703125, 999.91650390625, 115.267822265625, 48.26531982421875], [576.8048095703125, 765.8109130859375, 249.61666870117188, 66.5152587890625], [183.2305450439453, 446.08441162109375, 183.80892944335938, 113.84066772460938], [543.5894775390625, 882.1089477539062, 240.73626708984375, 55.653564453125], [267.8492736816406, 194.3089599609375, 215.689453125, 64.29965209960938], [518.5321655273438, 195.34548950195312, 238.0396728515625, 60.63056945800781], [309.5284118652344, 242.46510314941406, 314.5657958984375, 50.470611572265625], [665.92724609375, 242.24551391601562, 243.344482421875, 41.83235168457031], [700.2841186523438, 292.3075256347656, 206.8282470703125, 44.94488525390625], [394.9449462890625, 154.95631408691406, 242.5352783203125, 66.18209075927734], [589.929931640625, 933.2787475585938, 388.7536315917969, 52.8656005859375], [508.19024658203125, 1088.6424560546875, 321.0772705078125, 48.43603515625], [256.26385498046875, 299.1544189453125, 197.3598175048828, 40.612060546875], [716.9022216796875, 985.1146240234375, 269.57843017578125, 53.25262451171875], [686.8961181640625, 503.74176025390625, 157.06201171875, 59.2840576171875], [710.1586303710938, 141.34036254882812, 310.881103515625, 51.94984436035156], [703.5232543945312, 1044.32763671875, 245.160400390625, 64.4019775390625]]\n",
            "Папка уже существует.\n",
            "20 20 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clear(\"parts of 1_pass_1.png\")"
      ],
      "metadata": {
        "id": "-ur0nqCaDV3M"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подсчёт метрик"
      ],
      "metadata": {
        "id": "dq4ObAj6A0Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_nearest_word(test_json, predicted_json):\n",
        "    with open(test_json, 'r') as f:\n",
        "        test = json.load(f)\n",
        "    with open(predicted_json, 'r') as f:\n",
        "        predicted = json.load(f)\n",
        "    result = {}\n",
        "    for i in range(len(test)):\n",
        "        min_dist = 1000000\n",
        "        for j in range(len(predicted)):\n",
        "            dist = editdistance.eval(test[i], predicted[j])\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                result[test[i]] = predicted[j]\n",
        "\n",
        "def compute_metric(mapping):\n",
        "    cer = 0\n",
        "    wer = 0\n",
        "    acc = 0\n",
        "    for key in mapping.keys():\n",
        "        cer += char_error_rate(key, mapping[key])\n",
        "        wer += char_error_rate(key, mapping[key])\n",
        "        acc += (1 - char_error_rate(key, mapping[key]))\n",
        "    cer /= len(mapping)\n",
        "    wer /= len(mapping)\n",
        "    acc /= len(mapping)\n",
        "    return cer, wer, acc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qIG34OeDZKV",
        "outputId": "1933e889-86d3-4082-8982-0da914557b53"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл <PIL.Image.Image image mode=RGBA size=68x14 at 0x7802416088E0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=28x10 at 0x780241609D50>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=68x14 at 0x78024137E110>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=28x10 at 0x78024137DC30>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=68x14 at 0x780241613700>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=68x14 at 0x780241609030>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=68x14 at 0x78024160A5C0>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=28x10 at 0x780241609300>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=28x10 at 0x78024160B580>.png удалён.\n",
            "Файл <PIL.Image.Image image mode=RGBA size=28x10 at 0x780241612170>.png удалён.\n"
          ]
        }
      ]
    }
  ]
}